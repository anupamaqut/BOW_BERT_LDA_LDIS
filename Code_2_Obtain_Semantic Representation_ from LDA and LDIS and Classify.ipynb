{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n#.............................Load The dense Matrix.................................\n\ndft= pd.read_csv('dense_matrix.csv')\n\n\ndft.loc[dft['Rating'] ==1, 'Rating'] = 0\ndft.loc[dft['Rating'] ==2, 'Rating'] = 0\ndft.loc[dft['Rating'] ==3, 'Rating'] = 1\ndft.loc[dft['Rating'] ==4, 'Rating'] = 2\ndft.loc[dft['Rating'] ==5, 'Rating'] = 2\n\ndftt= dft.astype(int)\ndocs=[]\nfor index, row in dftt.iterrows():\n    present_words_with_count = []\n    for i, value in enumerate(row):\n        for x in range(value):\n            present_words_with_count.append(i)\n    docs.append(present_words_with_count)\n\nfrom tqdm import tqdm\nimport numpy as np\n\n#............................Obtain sentiment TOPIC DISTRIBUTION OF THE REVIEWS WITH LDA...............................................................................\n\ndef distributiont1(t):\n    D = len(docs)  # number of documents\n    V = dftt.shape[1]  # size of the vocabulary\n    T = t  # number of topics\n\n    alpha = 1 / T  # the parameter of the Dirichlet prior on the per-document topic distributions\n    beta = 1 / T  # the parameter of the Dirichlet prior on the per-topic word distribution\n\n    z_d_n = [[0 for _ in range(len(d))] for d in docs]  # z_i_j\n    theta_d_z = np.zeros((D, T))\n    phi_z_w = np.zeros((T, V))\n    n_d = np.zeros((D))\n    n_z = np.zeros((T))\n\n    ## Initialize the parameters\n    # m: doc id\n    for d, doc in enumerate(docs):\n        # n: id of word inside document, w: id of the word globally\n        for n, w in enumerate(doc):\n            # assign a topic randomly to words\n            z_d_n[d][n] = n % T\n            # get the topic for word n in document d\n            z = z_d_n[d][n]\n            # keep track of our counts\n            theta_d_z[d][z] += 1\n            phi_z_w[z, w] += 1\n            n_z[z] += 1\n            n_d[d] += 1\n\n    for iteration in tqdm(range(10)):\n        for d, doc in enumerate(docs):\n            for n, w in enumerate(doc):\n                # get the topic for word n in document m\n                z = z_d_n[d][n]\n\n                # decrement counts for word w with associated topic z\n                theta_d_z[d][z] -= 1\n                phi_z_w[z, w] -= 1\n                n_z[z] -= 1\n\n                # sample new topic from a multinomial according to our formula\n                p_d_t = (theta_d_z[d] + alpha) / (n_d[d] - 1 + T * alpha)\n                p_t_w = (phi_z_w[:, w] + beta) / (n_z + V * beta)\n                p_z = p_d_t * p_t_w\n                p_z /= np.sum(p_z)\n                new_z = np.random.multinomial(1, p_z).argmax()\n\n                # set z as the new topic and increment counts\n                z_d_n[d][n] = new_z\n                theta_d_z[d][new_z] += 1\n                phi_z_w[new_z, w] += 1\n                n_z[new_z] += 1\n    dfc = pd.DataFrame()\n    for i in range(D):\n        AR = pd.Series(theta_d_z[i] / sum(theta_d_z[i]))\n        dfc = pd.concat([dfc, pd.DataFrame([AR])], ignore_index=True)\n    dfc['sentiment'] = dft['Rating'].astype(int)\n    dfc = dfc.dropna()\n    X=NN_Classi(Linera_Discriminant_Analysis(dfc)[0],len(Linera_Discriminant_Analysis(dfc)[1]))\n\n    return X\n\n#........................Obtain Class Specific Features from LDIS..............................................\n\ndef Linera_Discriminant_Analysis(final_matrix):\n    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n    lda1= LDA()\n    x_vals= final_matrix.drop(['sentiment'], axis='columns')\n    y_vals=final_matrix.overall_rating\n    LDA_transformed= lda1.fit_transform(x_vals,y_vals)\n    topic_encoded_reviews= pd.DataFrame(LDA_transformed)\n    topic_encoded_reviews['sentiment']=dft['Rating']\n    topic_encoded_reviews.update(topic_encoded_reviews.select_dtypes(include=[np.number]).abs())# since NN doesnt get negative values.\n    \n    return topic_encoded_reviews,lda1.explained_variance_ratio_\n\n#.......................Classification with MLP.................................................................................\n\n\ndef NN_Classi(final_matrixx,ins):\n    import tensorflow as tf\n    from tensorflow import keras\n    from keras import models\n    from keras import layers\n    from tensorflow.keras.optimizers import Adam\n    from tensorflow.keras.losses import SparseCategoricalCrossentropy\n    from sklearn.model_selection import train_test_split\n    from tensorflow.keras import regularizers\n    from sklearn.metrics import classification_report, confusion_matrix, precision_score\n\n    model = keras.Sequential([keras.layers.Dense(32, input_shape=(ins,), activation='relu'),\n                              keras.layers.Dense(3, activation='softmax')\n                              ])\n    x_vals = final_matrixx.drop(['sentiment'], axis='columns')\n    y_vals = final_matrixx.sentiment\n    \n    #..................................Random sampling to get test results from random splits..............................\n    for i in range(20):\n\n        X_train_val, X_test, y_train_val, y_test = train_test_split(x_vals, y_vals, test_size=0.2, random_state=i,stratify=y)\n\n        # Step 2: Split train + validation into train and validation sets\n        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=i,stratify=y)\n\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train, y_train, epochs=20)\n     \n        y_pred = model.predict(X_test, verbose=2)\n\n    \n        predictions = np.argmax(y_pred, axis=1)\n        report = classification_report(y_test, predictions)\n        print(report)\n        \n        print(model.evaluate(X_test, y_test))\n\n\n    return model.evaluate(X_test, y_test)\n\n\ndistributiont1(768)# This is the requested topics for semantic representation.......","metadata":{},"execution_count":null,"outputs":[]}]}