{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ...................................Finetune Dataset with BERT and get embeddings of tokens to find similar terms in domain.............\nimport tensorflow as tf\n\nimport torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():\n    # Tell PyTorch to use the GPU.\n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")\n\nimport pandas as pd\n\n# Load the dataset into a pandas dataframe.\ndft = pd.read_csv(\"dataset.csv\")\n\n# Report the number of sentences.\nprint('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n\n# Display 10 random rows from the data.\n\ndft.loc[dft['Rating'] ==1, 'Rating'] = 0\ndft.loc[dft['Rating'] ==2, 'Rating'] = 0\ndft.loc[dft['Rating'] ==3, 'Rating'] = 1\ndft.loc[dft['Rating'] ==4, 'Rating'] = 2\ndft.loc[dft['Rating'] ==5, 'Rating'] = 2\n\ndft = dft[~dft.eq('').any(axis=1)]\ndft.reset_index(drop=True, inplace=True)\n\ndft = dft.dropna()\n\nsentences = dft.Review.values\nlabels = dft.Rating.values\n\nfrom transformers import BertTokenizer\n# Load the BERT tokenizer.\nprint('Loading BERT tokenizer...')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\nimport torch\n# Tokenize all of the sentences and map the tokens to thier word IDs.\ninput_ids = []\nattention_masks = []\n\n# For every sentence...\nfor sent in sentences:\n\n    encoded_dict = tokenizer.encode_plus(\n                        sent,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = 256,           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n\n    # Add the encoded sentence to the list.\n    input_ids.append(encoded_dict['input_ids'])\n\n    # And its attention mask (simply differentiates padding from non-padding).\n    attention_masks.append(encoded_dict['attention_mask'])\n\n# Convert the lists into tensors.\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(labels)\n\n# Print sentence 0, now as a list of IDs.\nprint('Original: ', sentences[0])\nprint('Token IDs:', input_ids[0])\n\nfrom torch.utils.data import TensorDataset, random_split\ndataset = TensorDataset(input_ids, attention_masks, labels)\ntrain_size = int(0.9 * len(dataset))\nval_size = len(dataset) - train_size\n# Divide the dataset by randomly selecting samples.\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))\n\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\nbatch_size = 16\n\ntrain_dataloader = DataLoader(\n            train_dataset,  # The training samples.\n            sampler = RandomSampler(train_dataset), # Select batches randomly\n            batch_size = batch_size # Trains with this batch size.\n        )\n\n# For validation the order doesn't matter, so we'll just read them sequentially.\nvalidation_dataloader = DataLoader(\n            val_dataset, # The validation samples.\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n            batch_size = batch_size # Evaluate with this batch size.\n        )\n\nnew_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\n\n# Load BertForSequenceClassification, the pretrained BERT model with a single\n# linear classification layer on top.\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n    num_labels = 3, # The number of output labels--2 for binary classification.\n                    # You can increase this for multi-class tasks.\n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = True, # Whether the model returns all hidden-states.\n)\n\n# Tell pytorch to run this model on the GPU.\nmodel.to(\"cuda\")\n\noptimizer = AdamW(model.parameters(),\n                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                )\n\nfrom transformers import get_linear_schedule_with_warmup\n\nepochs = 10\n\ntotal_steps = len(train_dataloader) * epochs\n\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)\n\nimport numpy as np\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\n\nimport random\nimport numpy as np\nimport torch\n\nseed_val = 42 # this is updated with each iteration of test samples\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\ntraining_stats = []\n\nearly_stopping_patience = 3  # Number of epochs to wait before stopping\nbest_validation_loss = float('inf')\nearly_stopping_counter = 0\ntoken_emb = []\n\nfor epoch_i in range(0, epochs):\n    token_embeddings_train = []\n\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    total_train_loss = 0\n\n    model.train()\n\n    for step, batch in enumerate(train_dataloader):\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        model.zero_grad()\n\n        outputs = model(b_input_ids,\n                        token_type_ids=None,\n                        attention_mask=b_input_mask,\n                        labels=b_labels,\n                        return_dict=True\n                        )\n\n        loss = outputs.loss\n        logits = outputs.logits\n\n        total_train_loss += loss.item()\n\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n\n        scheduler.step()\n\n    avg_train_loss = total_train_loss / len(train_dataloader)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    model.eval()\n\n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    nb_eval_steps = 0\n\n    for batch in validation_dataloader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        with torch.no_grad():\n            outputs = model(b_input_ids,\n                            token_type_ids=None,\n                            attention_mask=b_input_mask,\n                            labels=b_labels, return_dict=True)\n\n        loss = outputs.loss\n        logits = outputs.logits\n\n        total_eval_loss += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n\n    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n\n    avg_val_loss = total_eval_loss / len(validation_dataloader)\n\n    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n\n    if avg_val_loss < best_validation_loss:\n        best_validation_loss = avg_val_loss\n        early_stopping_counter = 0\n\n    else:\n        early_stopping_counter += 1\n        if early_stopping_counter >= early_stopping_patience:\n            print(\"Early stopping triggered! No improvement in validation loss for {} epochs.\".format(\n                early_stopping_patience))\n            break\n\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy\n        }\n    )\nprint(\"\")\nprint(\"Training complete!\")\n\n# ............................................... Function to Obtain BERT Token Embeddings for terms......................................\n\ndef get_bert_embeddings(dfrows):\n    word_ids_list=[]\n    embeddings_list=[]\n    word_names_list=[]\n    for text in dfrows:\n        tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n        tokens = {key: value.to(model.device) for key, value in tokens.items()}\n        with torch.no_grad():\n            outputs = model(**tokens)\n        word_ids = tokens[\"input_ids\"].tolist()[0]\n        hidden_states = outputs.hidden_states\n        embeddings = hidden_states[-1]\n        word_names = tokenizer.convert_ids_to_tokens(word_ids)\n        word_ids_list.append(word_ids)\n        embeddings_list.append(embeddings)\n        word_names_list.append(word_names)\n    return zip(word_ids_list, word_names_list, embeddings_list)\n\n\n# ............................................... Preprocessing to eliminate noise.......................................\ndef pre_preprocess(text):\n    import string\n    import nltk\n    nltk.download('punkt')\n    nltk.download('popular')\n    from nltk.corpus import stopwords\n    from nltk.tokenize import word_tokenize\n\n    # Sample text\n    #text = \"Hello, this is an example sentence! It includes punctuation and stopwords.\"\n    textx= text\n    # Remove punctuation\n    textx = textx.translate(str.maketrans('', '', string.punctuation)).replace(',', '')\n\n\n    # Tokenize the text\n    tokens = word_tokenize(textx)\n\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n\n    # Join the tokens back into a single string\n    processed_text = \" \".join(tokens)\n\n    return processed_text\n\n#.........................................Extract token embeddings of samples..............................................................\ndfttt= dft\nimport torch\nreview_list2 = (dfttt['Review'].astype(str)).tolist()\nzip2= get_bert_embeddings(review_list2)\n\nzipped_list1 = list(zip2)\nunzipped_list11, unzipped_list22,unzipped_list33 = zip(*zipped_list1)\n\nidl1= list(unzipped_list11)\nnl1 = list(unzipped_list22)\nembl1 = list(unzipped_list33)\n\ncolumn_names = ['No', 'reviews_given','senti_words','all_words','senti_word_synonyms','sep_words','Rating']\nsenti_df1 = pd.DataFrame(columns = column_names)\n\n#......................... Initial pre-processing to remove special tokens like cls and punctuations. Then create reviews with preprocessed text as reviews_given.................\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nwords_to_remove = ['[PAD]', '[SEP]','.','[CLS]','\\\\','/','(',')',',','\\'']\nappended_list=[]\nk=0\nfor list1 in nl1:\n    filtered_word_list = [word for word in list1 if word not in words_to_remove]\n    token_string = \", \".join(filtered_word_list)\n    preprocessed_reviews= pre_preprocess(token_string)\n\n    x= preprocessed_reviews.split(\" \")\n    appended_list.append(x)\n    if senti_df1.empty:\n        senti_df1.at[k, 'No'] = k\n        senti_df1.at[k, 'reviews_given']= preprocessed_reviews\n    else:\n        senti_df1.at[k, 'No'] = k\n        senti_df1.at[k, 'reviews_given']= preprocessed_reviews\n\n    k=k+1\n\n#...........................Preprocessing to extract adjectives and adverbs.Then create body_new column..................................................\nimport spacy\nnlp = spacy.load('en_core_web_trf')\ndef preprocess(text):\n    doc= nlp(text)\n    x=[token.text.lower() for token in doc if token.pos_ == \"ADJ\" or token.pos_ == \"ADV\"]\n    return ' '.join(x)\n\nsenti_df1['body_new']=senti_df1.reviews_given.astype(str).apply(preprocess)\nsenti_df1['Rating'] = dfttt['Rating']\n\nnew_list = appended_list\nsenti_df=senti_df1\nnl= nl1\nembl= embl1\n\nflattened_list = [item for sublist in new_list for item in sublist]\nunique_word_set = set(flattened_list)\n\n# Convert the set back to a list\nunique_word_list = list(unique_word_set)\nsenti_df = pd.concat([senti_df, pd.DataFrame(columns=unique_word_list)])\nwords_string = ' '.join(unique_word_list)\n\n\nxxx= preprocess(words_string)\nxxx_lict= xxx.split(' ')\n\ncommon_values = list(set(unique_word_list) & set(xxx_lict))\n\n#.....................................Seperate out the sentiment POS terms and the POS terms appeard in original reviews similar to such extracted terms. This is because preprocessing can make steming of tokens etc......\nimport math\nimport numpy as np\nimport re\nfor i in range(len(nl)):\n    adjdic={}\n    dic={}\n    dic1={}\n    for j in nl[i]:\n        if j in senti_df.columns:\n            indexes_of_value = [index for index, value in enumerate(nl[i]) if value == j]\n            values_at_indexes = [embl[i][0][index] for index in indexes_of_value]\n            if senti_df.empty:\n                senti_df.at[i, j] = values_at_indexes\n            else:\n                senti_df.at[i, j] = values_at_indexes\n            key1 = f'{j}'\n            value1 = values_at_indexes\n            dic1[key1] = value1\n            if len(j) > 1:\n                pattern = re.compile(rf'\\b{re.escape(j)}\\b', re.IGNORECASE)\n                match = re.search(pattern, senti_df['body_new'][i])\n\n                if match:\n                    key = f'{j}'\n                    value = values_at_indexes\n                    dic[key] = value\n                    k=f'{j}'\n                    v=len(values_at_indexes)\n                    adjdic[k]=v\n    senti_df.at[i, 'senti_words']=dic\n    senti_df.at[i, 'all_words']=dic1\n    senti_df.at[i, 'senti_word_synonyms']=adjdic\n\nselected_columns = ['reviews_given','senti_words','all_words','body_new','senti_word_synonyms','sep_words','Rating']\nnew_df = senti_df[selected_columns]\n\n\n# ...........................................Seperate out the non Sentiment POS terms to find similar terms....................................\n\nimport warnings\n\n# Ignore all warnings\nwarnings.filterwarnings('ignore')\n\npd.options.mode.chained_assignment = None\ndef compare_dictionaries(dict1, dict2):\n    different_dict = {}\n\n    if len(dict1) != len(dict2):\n        extra_keys = set(dict1) ^ set(dict2)\n        for key in extra_keys:\n            if key in dict1:\n                different_dict[key] = dict1[key]\n            else:\n                different_dict[key] = dict2[key]\n\n    return different_dict\n\nfor i in range(len(new_df)):\n    dicx= new_df['senti_words'][i]\n    dicy=new_df['all_words'][i]\n    new_df['sep_words'][i]=compare_dictionaries(dicx, dicy)\n\ncolumns_to_exclude = ['reviews_given', 'senti_words', 'all_words', 'body_new', 'senti_word_synonyms','No','sep_words','Rating']\n\n\n\n#....................................... Create a new DataFrame without the excluded columns.................................\n\nnewest_df = senti_df[[col for col in senti_df.columns if col not in columns_to_exclude]]\n\ndf_filled=newest_df.fillna(0)\ndf_filled\n\nfinal_df = df_filled.copy()\nfinal_df = final_df.astype(object)\nfor index, row in final_df.iterrows():\n    for i, value in enumerate(row):\n        if(value!=0):\n            column_name = final_df.columns[i]\n            final_df.at[index, column_name]= len(value)\nfinal_df\n\nsentiment_data = final_df[common_values]\nsentiment_data\n\nsentiment_data['sep_words'] = new_df['sep_words']\n\n#....................................................# Find similar non sentiment words from other terms with different cosine similarity tresholds....................\n\ndef similar_t(cvd,dffilled):\n    cvd = cvd.astype(object)\n    dffilled = dffilled.astype(object)\n    k=0\n    num=0\n    for index, row in cvd.iterrows():\n        for i, value in enumerate(row):\n            if value == 0:\n                column_name = cvd.columns[i]\n                non_zero_values_list = dffilled[dffilled[column_name] != 0][column_name].tolist()\n                num=0\n                for index1, list_val in enumerate(non_zero_values_list):\n                    syn_set=list_val\n                    for index1, list_val1 in enumerate(syn_set):\n                        sub_synset=list_val1\n                        for key, value in cvd['sep_words'][index].items():\n                            sep_w_syn_set=value\n                            for index2, list_val2 in enumerate(sep_w_syn_set):\n                                cosine_similarity_value = torch.nn.functional.cosine_similarity(sub_synset, list_val2, dim=0)\n                                if cosine_similarity_value>0.8:\n                                    num=num+1\n            cvd.at[k,column_name] = num\n        k=k+1\n    return cvd\n\n\na= sentiment_data\na1= df_filled\na.reset_index(drop=True, inplace=True)\nadf=similar_t(a,a1)\nadf.to_csv('dense_matrix.csv') # Save the dense matrix to csv file.","metadata":{},"execution_count":null,"outputs":[]}]}